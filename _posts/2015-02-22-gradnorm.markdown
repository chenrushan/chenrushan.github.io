---
layout: post
title: About Gradient Norm
categories: numop
tags: gradient norm
---

我们知道，对于 convex function 而言，最优值点出的 gradient = 0 (当然 gradient
norm 也是 0)，但这并不意味着

* gradient norm 越小的点离最优值点越近

    考虑函数 $f(\b{x}) = \frac{1}{2}(x\_1^2 + 5x\_2^2)$，
    $\Vert f'(\b{x}) \Vert^2 = x\_1^2 + 25 x\_2^2$
    
    令 $\b{x}\_1 = (0, \sqrt{5}), \b{x}\_2 = (5, 0), \b{x}^* = (0, 0)$

    显然 $\b{x}^*$ 为 $f(\b{x})$ 的最优值点且
    $\Vert \b{x}\_1 - \b{x}^* \Vert < \Vert \b{x}\_2 - \b{x}^\* \Vert$，但是
    $\Vert f'(\b{x}\_1) \Vert^2 > \Vert f'(\b{x}\_2) \Vert^2$

* gradient norm 越小的点 $f(\b{x})$ 与 $f(\b{x}^*)$ 差距就越小

    同样考虑上述 $f(\b{x})$，令 $\b{x}\_1 = (0, \sqrt{4}), \b{x}\_2 = (5, 0)$

    则有 $f(\b{x}\_1) - f(\b{x}^*) < f(\b{x}\_2) - f(\b{x}^\*)$，但是
    $\Vert f'(\b{x}\_1) \Vert^2 > \Vert f'(\b{x}\_2) \Vert^2$

