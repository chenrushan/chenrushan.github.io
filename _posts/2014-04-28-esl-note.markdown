---
layout: post
title: Note of The Element of Statistical Learning
categories: ml
---

### 公式解析

#### (2.12) -> (2.13)

$$
\begin{align}
& E\_{Y|X}([Y-c]^2 | X=x)  \\\\
= & E\_{Y|X}(Y^2 - 2cY + c^2 | x)  \\\\
= & E\_{Y|X}(Y^2 | x) - E\_{Y|X}(2cY | x) + c^2  \\\\
\end{align}
$$

$$
\begin{align}
\frac{\partial E\_{Y|X}([Y-c]^2 | X=x)}{\partial c} = 2c - 2E\_{Y|X}(Y|X = x) = 0
\end{align}
$$

$$
\therefore f(x) = c = E\_{Y|X}(Y|X=x)
$$

#### (2.26)

下面我用 $\varepsilon$ 表示针对一个样本，粗体的 $\boldsymbol{\varepsilon}$ 表示针对所有样本，所以是个 nx1 的向量。

(2.26) 下面的 $\hat{y}\_0 = x\_0^T \hat{\beta}$ 等价于 $\hat{y}\_0 = x\_0^T \beta + \sum\_{i=1}^{N} l\_i(x\_0) \boldsymbol{\varepsilon}\_i$ where $l\_i(x\_0)$ is the ith element of $\boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X})^{-1}x\_0$

后面那个 $\hat{y}\_0$ 用 matrix 表示就是 $\hat{y}\_0 = x\_0^T \beta + (\boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X})^{-1}x\_0)^T \boldsymbol{\varepsilon}$

对 $Y = X^T \beta + \varepsilon$ 应用 least square 即 $(\boldsymbol{X}\hat{\beta} - \boldsymbol{X}\beta - \boldsymbol{\varepsilon})^T(\boldsymbol{X}\hat{\beta} - \boldsymbol{X}\beta - \boldsymbol{\varepsilon})$，展开求偏导，可得 $(\hat{\beta} - \beta) = (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{X}^T \boldsymbol{\varepsilon}$，注意到 $\boldsymbol{X}^T\boldsymbol{X}$ 是 symmetric matrix，所以 $(\boldsymbol{X}^T\boldsymbol{X})^{-1} = ((\boldsymbol{X}^T\boldsymbol{X})^{-1})^T$，则有 $(\hat{\beta} - \beta) = (\boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X})^{-1})^T \boldsymbol{\varepsilon}$，等式两边同时于 $x\_0$ 做内积，很容易就推出上面的式子。

#### (2.25) and (2.27)

公式 (2.25) 和 (2.27) 都是计算 MSE 的，不同的是，(2.25) 对应的模型是个 determinstic function $Y = f(X)$，而 (2.27) 对应的是 $Y = f(X) + \varepsilon, \; \varepsilon \sim N(0, \delta^2)$，所以是个 non-determinstic function。这也造成了 (2.25) 和 (2.27) 不同，对于 determinstic function，$MSE(x\_0) = E\_{\mathcal{T}}(y\_0 - \hat{y}\_0)^2$，而对于 non-determinstic function 就要外面再套一个 $E\_{y\_0 | x\_0}$ 变成 $E\_{y\_0 | x\_0}E\_{\mathcal{T}}(y\_0 - \hat{y}\_0)^2$。

上面公式中 $E\_{\mathcal{T}}(y\_0 - \hat{y}\_0)^2$ 变化的是 $\hat{y}\_0$，对于不同的 $\mathcal{T}$，$\hat{y}\_0$ 是不同的，对于给定的一个 $\mathcal{T}$，$\hat{y}\_0$ 是确定的。

* (2.25) 的推导的 trick 是 

  $$MSE(x\_0) = E\_{\mathcal{T}}(y\_0 - \hat{y}\_0)^2 = E\_{\mathcal{T}}((y\_0 - E\_{\mathcal{T}}[\hat{y}\_0]) + (E\_{\mathcal{T}}[\hat{y}\_0] - \hat{y}\_0))^2$$

* (2.27) 的 trick 要多一点，为了方便，我用 $E$ 代替 $E\_{y\_0 | x\_0}E\_{\mathcal{T}}$

  $$
  \begin{align}
  EPE(x\_0) = & E(y\_0 - \hat{y}\_0)^2 \\\\
  = & E((y\_0 - f(x\_0)) + (f(x\_0) - \hat{y}\_0))^2 \\\\
  = & E(y\_0 - f(x\_0))^2 + E(f(x\_0) - \hat{y}\_0))^2 + 2E((y\_0 - f(x\_0))(f(x\_0) - \hat{y}\_0))
  \end{align}
  $$
  
  因为 $f(x\_0)$ 是 determinstic 的，所以
  
  $$E(y\_0 f(x\_0)) = f(x\_0)E(y\_0) = f^2(x\_0)$$
  $$E(y\_0\hat{y}\_0 - f(x\_0)\hat{y}\_0) = E(\varepsilon \hat{y}\_0) = E\_{y\_0 | x\_0}E\_{\mathcal{T}}(\varepsilon \hat{y}\_0) = E\_{y\_0 | x\_0}(\varepsilon E\_{\mathcal{T}}(\hat{y}\_0)) = E\_{y\_0 | x\_0}(\varepsilon) E\_{\mathcal{T}}(\hat{y}\_0) = 0$$

  所以 $EPE(x\_0) = E(y\_0 - f(x\_0))^2 + E(f(x\_0) - \hat{y}\_0))^2$，对后面一项再应用 (2.25) 推导的 trick，就得到了

  $$EPE(x\_0) = \delta^2 + Var(\hat{y}\_0) + Bias^2(\hat{y}\_0)$$

  根据上面一小节关于 (2.26) 中给出的 $\hat{y}\_0$ 的公式可知 $E(\hat{y}\_0) = x\_0^T \beta$，所以 $Bias(\hat{y}\_0) = 0$。

  $Var(\hat{y}\_0) = \delta^2 E(x\_0^T(\boldsymbol{X}^T \boldsymbol{X})^{-1}x\_0)$ 是这么推导的 (想了好久终于想出来，I'm a genius)

  令 $\boldsymbol{v} = (\boldsymbol{X}(\boldsymbol{X}^T\boldsymbol{X})^{-1}x\_0)$

  $$
  \begin{align}
  Var(\hat{y}\_0) = & Var(x\_0^T \beta + \boldsymbol{v}^T \boldsymbol{\varepsilon}) \\\\
  = & E((\boldsymbol{v}^T \boldsymbol{\varepsilon})^2) \\\\
  = & E((\sum\_i v\_i \varepsilon\_i)^2) \\\\
  = & E(\sum\_i v\_i^2 \varepsilon\_i^2) (\text{since if} \; i \neq j \; \varepsilon\_i \; \text{and} \; \varepsilon\_j \; \text{are iid, so} \; E(\varepsilon\_i \varepsilon\_j) = 0) \\\\
  = & \delta^2 E(\sum\_i v\_i^2) \\\\
  = & \delta^2 E(\boldsymbol{v}^T \boldsymbol{v}) \\\\
  = & \delta^2 E(x\_0^T(\boldsymbol{X}^T \boldsymbol{X})^{-1}x\_0)
  \end{align}
  $$

  另外意外想到的一个公式是 $tr(\boldsymbol{v}\boldsymbol{v}^T) = \boldsymbol{v}^T\boldsymbol{v}$

#### (2.28)

在推导 (2.28) 前先明确下面的公式 (只要展开即可证明)

$$x^T A x = \sum\_{i}\sum\_{j} A\_{ij} x\_i x\_j$$
$$\sum\_{i}\sum\_{j} A\_{ij} B\_{ji} = trace(AB) = trace(BA)$$

根据上面两个公式也可得到 $x^T A x = tr(A (xx^T))$

为了方便，下面的公式里就不用 $x\_0$ 了，直接用 $x$

$$
\begin{align}
& E\_x (x^T Cov(X)^{-1} x) \\\\
= & E\_x (\sum\_{i}\sum\_{j} Cov(X)^{-1}\_{ij} x\_i x\_j) \\\\
= & \sum\_{i}\sum\_{j} Cov(X)^{-1}\_{ij} E\_x (x\_i x\_j) \\\\
= & \sum\_{i}\sum\_{j} Cov(X)^{-1}\_{ij} Cov(x\_i, x\_j) (\because E(X) = 0) \\\\
= & trace(Cov(X)^{-1} Cov(x)) \\\\
= & trace(I) \\\\
= & p
\end{align}
$$

#### (3.4)

(3.4) 中给出了 Hessian matrix，目的是为了说明 RSS 是个 strictly convex function，因为对于 strictly convex function 其对应的 hessian matrix 是 pd 的。这样通过一阶导求出的结果就是 global mininum。

关于 $\boldsymbol{X}^T\boldsymbol{X}$ 的正定性很容易证明，$x^T\boldsymbol{X}^T\boldsymbol{X}x = (\boldsymbol{X}x)^T (\boldsymbol{X}x) \geq 0$，所以 $\boldsymbol{X}^T\boldsymbol{X}$ 一定是 psd 的，如果 $\boldsymbol{X}$ 是 full column rank 的，那对于所有非 0 的 $x$，$\boldsymbol{X}x$ 就不可能是 0 (因为 $\boldsymbol{X}x = \sum\_{i} \boldsymbol{X}\_i x\_i$，其中 $\boldsymbol{X}\_i$ 表示第 $i$ 列)，则 $\boldsymbol{X}^T\boldsymbol{X}$ 就是 pd 的。

### Statistics Basic

* What regression is all about is try to figure out why a particular variable varies (from [here](https://www.youtube.com/watch?v=aq8VU5KLmkY))，这里的 variable 指的是 dependent variable，所以知道 dependent variable 的 variance 有多少被我们的 model cover 就很有意义，所以有了像 $R^2$ 这样的指标，$R^2$ 表示 the proportion of total variance that is taken up by the model (equal to SSR/SST), the lower the SSE is, the higher the $R^2$。

* [Degree of freedom](https://www.youtube.com/watch?v=4otEcA3gjLk)

### Good Sites

1. [Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html)

