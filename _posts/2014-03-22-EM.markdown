---
layout: post
title: Expectation Maximization
categories: ml
tags: EM, Expectation Maximization
---

### Introduction

给定数据 $X$，一种常用的估计模型参数 $\theta$ 的方法是 maximum likelihood，即：

$$\theta = \underset{\theta}{\arg\max} \;\; p(X|\theta)$$

这里的 $X$ 是我们可见的，即 visible variable。如果给定的问题涉及到 hidden variable，ML 就需要引入新变量，即：

$$\theta = \underset{\theta}{\arg\max} \;\; p(X|\theta) = \underset{\theta}{\arg\max} \;\; \int p(X, z|\theta) dz$$

其中 $z$ 对应 hidden variable。在很多情况下，引入积分后问题会变得不可求解，这也是为什么需要 EM 算法的原因。EM 尝试用迭代的方式去求解 $\theta$，从而避免掉积分操作。

另外，为了计算方便，我们通常并不直接操作 $p(X|\theta)$，而是用 $\log$ 形式代替，即：

$$\theta = \underset{\theta}{\arg\max} \; \log p(X|\theta)$$

为了使用方便，下面记 $L(\theta) = \log p(X|\theta)$

### EM Iteration

假设第 $n$ 轮迭代得到的参数是 $\theta\_n$，由于我们的目标是最大化 $L(\theta)$，因此我们希望下一轮迭代得到的 $\theta$ 可以进一步提升 $L(\theta)$。换句话说，在第 $n+1$ 轮迭代时，希望能找到 $\theta$ 使得 $L(\theta) > L(\theta\_n)$。

首先明确一点，我们不可能通过直接优化 $L(\theta) - L(\theta\_n)$ 得到下一轮的 $\theta$，因为优化 $L(\theta) - L(\theta\_n)$ 等价于优化 $L(\theta)$ ($L(\theta\_n)$是常数，在优化中不起作用)，这个前面已经说了不好优化，好优化就不用 EM 算法了。为了得到一个更好的 $\theta$， **EM 算法的做法是先得到 $L(\theta) - L(\theta\_n)$ 的一个下界，然后去优化这个下界来得到一个更优的 $\theta$** (当然这个下界通常是易于优化的)。

下面的推导首先给出 $L(\theta) - L(\theta\_n)$ 的下界并证明可以通过优化这个下界得到更优的 $\theta$。

推导过程中用到了 concave function 的一个性质：

<blockquote>
If $f(x)$ is a concave function, $\forall \sum_i a_i = 1 \;\; f(\sum_{i}a_i x_i) \geq \sum_i a_i f(x_i)$
</blockquote>

特别的，如果 $f(x) = \log x$，则有 $\log \sum\_i a\_i x\_i \geq \sum\_i a\_i \log x\_i \;\; \forall \sum\_i a\_i = 1$

#### 得到下界

利用 concave function 的性质，可得：

$$
\begin{align}
& L(\theta) - L(\theta\_n) \\\\
= & \log p(X|\theta) - \log p(X|\theta\_n) \\\\
= & \log \sum\_z p(X, z|\theta) - \log p(X|\theta\_n) \\\\
= & \log \sum\_z p(X, z|\theta) \frac{p(z|X, \theta\_n)}{p(z|X, \theta\_n)} - \log p(X|\theta\_n) \\\\
\geq & \sum\_z p(z|X, \theta\_n) \log \frac{p(X, z|\theta)}{p(z|X, \theta\_n)} - \log p(X|\theta\_n) \\\\
= & \sum\_z p(z|X, \theta\_n) \log \frac{p(X, z|\theta)}{p(z|X, \theta\_n)p(X|\theta\_n)} \\\\
= & \sum\_z p(z|X, \theta\_n) \log \frac{p(X, z|\theta)}{p(X, z|\theta\_n)} \\\\
= & \Delta(\theta | \theta\_n)
\end{align}
$$

这样我们就得到了 $L(\theta) - L(\theta\_n)$ 的下界 $\Delta(\theta | \theta\_n)$。推导的核心的是 **第三个等式引入了 $\frac{p(z|X, \theta\_n)}{p(z|X, \theta\_n)}$** 。

#### 优化下界

接下来证明通过优化下界可以得到一个更好的 $\theta$。

当 $\theta = \theta\_n$ 时，我们有：

$$
\begin{align}
& \Delta(\theta\_n | \theta\_n) \\\\
= & \sum\_z p(z|X, \theta\_n) \log \frac{p(X, z|\theta\_n)}{p(X, z|\theta\_n)} \\\\
= & \sum\_z p(z|X, \theta\_n) \log 1 \\\\
= & 0
\end{align}
$$

因此，如果 $\theta\_{n+1} = \underset{\theta}{\arg\max}\; \Delta(\theta|\theta\_n)$ 则必有 $\Delta(\theta\_{n+1}|\theta\_n) \geq 0$，也就是 $L(\theta\_{n+1}) - L(\theta\_n) \geq 0$，这样也就得到了一个更好的 $\theta$。

#### E-step & M-step

下面得到 $\underset{\theta}{\arg\max}\; \Delta(\theta|\theta\_n)$ 的具体形式。

$$
\begin{align}
\theta\_{n+1} = & \underset{\theta}{\arg\max}\; \Delta(\theta|\theta\_n) \\\\
= & \underset{\theta}{\arg\max}\; \sum\_z p(z|X, \theta\_n) \log \frac{p(X, z|\theta)}{p(X, z|\theta\_n)} \\\\
= & \underset{\theta}{\arg\max}\; \sum\_z p(z|X, \theta\_n) \log p(X, z|\theta) \\\\
= & \underset{\theta}{\arg\max}\; E\_{z|X, \theta\_n} (\log p(X, z|\theta))
\end{align}
$$

这样也就得到了 E-step 和 M-step:

<blockquote>
<ul>
<li>E-step</li>

得到 $E\_{z|X, \theta\_n} (\log p(X, z|\theta))$
<li>M-step</li>

计算 $\underset{\theta}{\arg\max}\; E\_{z|X, \theta\_n} (\log p(X, z|\theta))$
</ul>
</blockquote>

下面给出两个具体的例子，分别是 Hidden Markov Model (HMM) 和 Probabilistic Latent Semantic Analysis (PLSA) 的训练。

### PLSA


